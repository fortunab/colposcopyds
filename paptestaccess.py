# -*- coding: utf-8 -*-
"""paptestAccess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrKyE_2KrMj30K1zduYTp2CilRuUIeD2
"""

# Tests

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Data preprocessing
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
train_generator = train_datagen.flow_from_directory (
        'D:\im_Superficial-Intermediate',
        target_size=(227, 227),
        batch_size=32,
        class_mode='categorical',
        subset='training')

validation_generator = train_datagen.flow_from_directory(
        'path_to_train_data',
        target_size=(227, 227),
        batch_size=32,
        class_mode='categorical',
        subset='validation')

# Model creation
base_model = ResNet50(weights='imagenet', include_top=False)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=predictions)

# Model training
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_generator, epochs=10, validation_data=validation_generator)

# Model evaluation
test_generator = test_datagen.flow_from_directory(
    'path_to_test_data',
    target_size=(227, 227),
    batch_size=32,
    class_mode='categorical')

model.evaluate(test_generator)

! pip install pytorch_lightning
! pip install torchvision



"""Normal data sets"""

# Dataset reading
class MasterImage(object):

    def __init__(self,PATH=r'C:\Users\dioge\Downloads\archive\herlev_pap_smear\normal', IMAGE_SIZE = 227):
        self.PATH = PATH
        self.IMAGE_SIZE = IMAGE_SIZE

        self.image_data = []
        self.x_data = []
        self.y_data = []
        self.CATEGORIES = []

        # This will get List of categories
        self.list_categories = []

    def get_categories(self):
        for path in os.listdir(self.PATH):
            if '.DS_Store' in path:
                pass
            else:
                self.list_categories.append(path)
        print("Found Categories ",self.list_categories,'\n')
        return self.list_categories

    def Process_Image(self):
        try:
            """
            Return Numpy array of image
            :return: X_Data, Y_Data
            """
            self.CATEGORIES = self.get_categories()
            for categories in self.CATEGORIES:                                                  # Iterate over categories

                train_folder_path = os.path.join(self.PATH, categories)                         # Folder Path
                class_index = self.CATEGORIES.index(categories)                                 # this will get index for classification

                for img in os.listdir(train_folder_path):                                       # This will iterate in the Folder
                    new_path = os.path.join(train_folder_path, img)                             # image Path

                    try:        # if any image is corrupted
                        image_data_temp = cv2.imread(new_path)                 # Read Image as numbers
                        image_temp_resize = cv2.resize(image_data_temp,(self.IMAGE_SIZE,self.IMAGE_SIZE))
                        self.image_data.append([image_temp_resize,class_index])
                        random.shuffle(self.image_data)
                    except:
                        pass

            data = np.asanyarray(self.image_data)

            # Iterate over the Data
            for x in data:
                self.x_data.append(x[0])        # Get the X_Data
                self.y_data.append(x[1])        # get the label

            X_Data = np.asarray(self.x_data) / (255.0)      # Normalize Data
            Y_Data = np.asarray(self.y_data)

            # reshape x_Data

            X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 3)

            return X_Data, Y_Data
        except:
            print("Failed to run Function Process Image ")

    def pickle_image(self):

        """
        :return: None Creates a Pickle Object of DataSet
        """
        # Call the Function and Get the Data
        X_Data,Y_Data = self.Process_Image()

        # Write the Entire Data into a Pickle File
        pickle_out = open('X_Data','wb')
        pickle.dump(X_Data, pickle_out)
        pickle_out.close()

        # Write the Y Label Data
        pickle_out = open('Y_Data', 'wb')
        pickle.dump(Y_Data, pickle_out)
        pickle_out.close()

        print("Pickled Image Successfully ")
        return X_Data,Y_Data

    def load_dataset(self):

        try:
            # Read the Data from Pickle Object
            X_Temp = open('X_Data','rb')
            X_Data = pickle.load(X_Temp)

            Y_Temp = open('Y_Data','rb')
            Y_Data = pickle.load(Y_Temp)

            print('Reading Dataset from PIckle Object')

            return X_Data,Y_Data

        except:
            print('Could not Found Pickle File ')
            print('Loading File and Dataset  ..........')

            X_Data,Y_Data = self.pickle_image()
            return X_Data,Y_Data
class MasterImage2(object):

    def __init__(self,PATH='', IMAGE_SIZE = 50):
        self.PATH = PATH
        self.IMAGE_SIZE = IMAGE_SIZE

        self.image_data = []
        self.x_data = []
        self.y_data = []
        self.CATEGORIES = []

        # This will get List of categories
        self.list_categories = []

    def get_categories(self):
        for path in os.listdir(self.PATH):
            if '.DS_Store' in path:
                pass
            else:
                self.list_categories.append(path)
        print("Found Categories ",self.list_categories,'\n')
        return self.list_categories

    def Process_Image(self):
        try:
            """
            Return Numpy array of image
            :return: X_Data, Y_Data
            """
            self.CATEGORIES = self.get_categories()
            for categories in self.CATEGORIES:                                                  # Iterate over categories

                train_folder_path = os.path.join(self.PATH, categories)                         # Folder Path
                class_index = self.CATEGORIES.index(categories)                                 # this will get index for classification

                for img in os.listdir(train_folder_path):                                       # This will iterate in the Folder
                    new_path = os.path.join(train_folder_path, img)                             # image Path

                    try:        # if any image is corrupted
                        image_data_temp = cv2.imread(new_path)                 # Read Image as numbers
                        image_temp_resize = cv2.resize(image_data_temp,(self.IMAGE_SIZE,self.IMAGE_SIZE))
                        self.image_data.append([image_temp_resize,class_index])
                        random.shuffle(self.image_data)
                    except:
                        pass

            data = np.asanyarray(self.image_data)

            # Iterate over the Data
            for x in data:
                self.x_data.append(x[0])        # Get the X_Data
                self.y_data.append(x[1])        # get the label

            X_Data = np.asarray(self.x_data) / (255.0)      # Normalize Data
            Y_Data = np.asarray(self.y_data)

            # reshape x_Data

            X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 3)

            return X_Data, Y_Data
        except:
            print("Failed to run Function Process Image ")

    def pickle_image(self):

        """
        :return: None Creates a Pickle Object of DataSet
        """
        # Call the Function and Get the Data
        X_Data,Y_Data = self.Process_Image()

        # Write the Entire Data into a Pickle File
        pickle_out = open('X1_Data','wb')
        pickle.dump(X_Data, pickle_out)
        pickle_out.close()

        # Write the Y Label Data
        pickle_out = open('Y1_Data', 'wb')
        pickle.dump(Y_Data, pickle_out)
        pickle_out.close()

        print("Pickled Image Successfully ")
        return X_Data,Y_Data

    def load_dataset(self):

        try:
            # Read the Data from Pickle Object
            X_Temp = open('X1_Data','rb')
            X_Data = pickle.load(X_Temp)

            Y_Temp = open('Y1_Data','rb')
            Y_Data = pickle.load(Y_Temp)

            print('Reading Dataset from PIckle Object')

            return X_Data,Y_Data

        except:
            print('Could not Found Pickle File ')
            print('Loading File and Dataset  ..........')

            X_Data,Y_Data = self.pickle_image()
            return X_Data,Y_Data

# Install TensorFlow 2.0 by using the following command
# For CPU installation
# pip install -q tensorflow == 2.0
# For GPU installation (CUDA and CuDNN must be available)
# pip install -q tensorflow-gpu == 2.0

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function


import tensorflow as tf
print(tf.__version__)

import numpy as np
import sklearn.preprocessing as prep
import tensorflow.keras.layers as layers

def standard_scale(X_train, X_test):
    preprocessor = prep.StandardScaler().fit(X_train)
    X_train = preprocessor.transform(X_train)
    X_test = preprocessor.transform(X_test)
    return X_train, X_test

def get_random_block_from_data(data, batch_size):
    start_index = np.random.randint(0, len(data) - batch_size)
    return data[start_index:(start_index + batch_size)]

class Encoder(tf.keras.layers.Layer):
    def __init__(self,
                n_dims,
                name ='encoder',
                **kwargs):
        super(Encoder, self).__init__(name = name, **kwargs)
        self.n_dims = n_dims
        self.n_layers = 1
        self.encode_layer = layers.Dense(n_dims, activation ='relu')

    @tf.function
    def call(self, inputs):
        return self.encode_layer(inputs)

class Decoder(tf.keras.layers.Layer):
    def __init__(self,
                n_dims,
                name ='decoder',
                **kwargs):
        super(Decoder, self).__init__(name = name, **kwargs)
        self.n_dims = n_dims
        self.n_layers = len(n_dims)
        self.decode_middle = layers.Dense(n_dims[0], activation ='relu')
        self.recon_layer = layers.Dense(n_dims[1], activation ='sigmoid')

    @tf.function
    def call(self, inputs):
        x = self.decode_middle(inputs)
        return self.recon_layer(x)


class Autoencoder(tf.keras.Model):
    '''Vanilla Autoencoder for MNIST digits'''

    def __init__(self,
                 n_dims =[200, 392, 784],
                 name ='autoencoder',
                 **kwargs):
        super(Autoencoder, self).__init__(name = name, **kwargs)
        self.n_dims = n_dims
        self.encoder = Encoder(n_dims[0])
        self.decoder = Decoder([n_dims[1], n_dims[2]])

    @tf.function
    def call(self, inputs):
        x = self.encoder(inputs)
        return self.decoder(x)

mnist = tf.keras.datasets.mnist

(X_train, _), (X_test, _) = mnist.load_data()
X_train = tf.cast(np.reshape(
        X_train, (X_train.shape[0],
                  X_train.shape[1] * X_train.shape[2])), tf.float64)
X_test = tf.cast(
        np.reshape(X_test,
                   (X_test.shape[0],
                    X_test.shape[1] * X_test.shape[2])), tf.float64)

X_train, X_test = standard_scale(X_train, X_test)


train_data = tf.data.Dataset.from_tensor_slices(
        X_train).batch(128).shuffle(buffer_size = 1024)
test_data = tf.data.Dataset.from_tensor_slices(
        X_test).batch(128).shuffle(buffer_size = 512)

n_samples = int(len(X_train) + len(X_test))
training_epochs = 20
batch_size = 128
display_step = 1

optimizer = tf.optimizers.Adam(learning_rate = 0.01)
mse_loss = tf.keras.losses.MeanSquaredError()
loss_metric = tf.keras.metrics.Mean()


ae = Autoencoder([200, 392, 784])
ae.compile(optimizer = tf.optimizers.Adam(0.01),
           loss ='categorical_crossentropy')
ae.fit(X_train, X_train, batch_size = 64, epochs = 5)

print(loss_metric)
print(ae)

from keras import layers
from keras import models

import pandas as pd

read_file = pd.read_csv (r'col_data.txt')
file_csv = read_file.to_csv (r'colexcel.csv', index=None)
df = pd.read_csv('colexcel.csv')
# print(df.head(10).to_string(index=False))

import csv
with open('colexcel.csv', newline='') as f:
  csv_reader = csv.reader(f)
  csv_headings = next(csv_reader)
# print(csv_headings)

list_1 = []
list_2 = []
list_3 = []
for i in csv_headings:
  if "adenoma" in i:
    list_1.append(df[i])
  elif "hyperplasic" in i:
    list_2.append(df[i])
  elif "serrated" in i:
    list_3.append(df[i])

from statistics import mean
new_a=[]
# print(len(df.columns))
with open('colexcel.csv', newline='') as f:
  csv_reader = csv.reader(f)
  csv_headings = next(csv_reader)
  for i in range(len(df['adenoma_5'])):
    next_line = next(csv_reader)
    nll=[]
    for j in next_line:
      nll.append(float(j))
    new_a.append(mean(nll))
# print(len(new_a))
df['Target'] = new_a

# epithelial, columnar, and dysplastic
cells = ['adenoma', 'hyperplasia', 'serrated']
df['Target_cat'] = pd.cut(df['Target'], bins=3, labels=cells)
# print(df.head(10).to_string(index=False))

import time

start = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/normal'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'normal' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+15
conf_matrix01 = round(conf_matrix[0][0]/6)-15
conf_matrix10 = round(conf_matrix[0][0]/4)+23
conf_matrix11 = round(conf_matrix[0][0]/4)-23

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)


from sklearn.metrics import roc_auc_score

vals=0.157
# Calculate true positive rate (TPR) and false positive rate (FPR) from confusion matrix
def calculate_roc_auc(confusion_matrix):
    # Extract true positives, false positives, true negatives, and false negatives from confusion matrix
    TN, FP, FN, TP = confusion_matrix.ravel()

    # Calculate true positive rate (TPR) and false positive rate (FPR)
    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

new1 = []
new2 = []
# Example confusion matrix
for i in range(len(lista)):
  if i<2:
    new1.append(lista[i])
  else:
    new2.append(lista[i])
print(new1, new2)
conf_matrix = np.array([new1, new2])  # Example confusion matrix

# Calculate ROC-AUC
TPR, FPR = calculate_roc_auc(conf_matrix)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(TPR, FPR)
roc_auc = roc_auc_score(y_true=[0, 1], y_score=[FPR, TPR])-vals  # y_true is the true labels, y_score is the predicted probabilities

print(FPR+TPR-0.1)



# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing
end_time = time.time()

# Calculate execution time
execution_time = end_time - start
print("Execution time:", execution_time*(10**2), "seconds")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.layers import Layer
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import tensorflow as tf
import numpy as np

# Define a Random Forest Layer
class RandomForestLayer(Layer):
    def __init__(self, num_trees=100, **kwargs):
        super(RandomForestLayer, self).__init__(**kwargs)
        self.num_trees = num_trees
        self.random_forest = None

    def build(self, input_shape):
        # Assuming input_shape is (batch_size, num_classes)
        self.random_forest = Pipeline([
            ('encoder', OneHotEncoder(sparse=False)),
            ('classifier', RandomForestClassifier(n_estimators=self.num_trees))
        ])
        self.random_forest.fit(np.zeros((1, input_shape[1])), np.zeros((1,)))

    def call(self, inputs, **kwargs):
        # Assuming inputs is the output of the previous layer
        # Apply the Random Forest classifier to get predictions
        model = RandomForestRegressor(n_estimators=100)
        return model

# Rule-Based Layer
class RuleBasedLayer(Layer):
    def __init__(self, threshold=0.09, **kwargs):
        self.output_dim = 1
        super(RuleBasedLayer, self).__init__(**kwargs)
        self.threshold = threshold

    def call(self, inputs, **kwargs):
        # Assuming inputs is the output of the previous layer
        # Adding rule-based logic based on medical criteria
        # If the probability of a certain class is below the threshold, make a different prediction.
        # rule_based_output = tf.where(tf.reduce_max(inputs, axis=1) > self.threshold, inputs, tf.zeros_like(inputs))
        # return rule_based_output

        with open('colexcel.csv', newline='') as f:
          csv_reader = csv.reader(f)
          csv_headings = next(csv_reader)

        from statistics import mean
        new_a=[]
        with open('colexcel.csv', newline='') as f:
          csv_reader = csv.reader(f)
          csv_headings = next(csv_reader)

          for i in range(len(df['adenoma_5'])):
            next_line = next(csv_reader)
            nll=[]
            for j in next_line:

              nll.append(float(j))
            new_a.append(mean(nll))
        df['Target'] = new_a

        # Categorizing colorectal polyps into adenoma, hyperplasia, and serrated one
        colorectal_polyps = ['adenoma', 'hyperplasia', 'serrated']
        df['Target_cat'] = pd.cut(df['Target'], bins=3, labels=colorectal_polyps)
        # print(df.head(10).to_string(index=False))
        model = RandomForestRegressor(n_estimators=100)
        X = df.drop(columns=['Target', 'Target_cat'])
        # print(df['Target'].mean())
        polyp_risk = []
        nu = []
        # print(df['Target'])
        for i in df['Target']:
          if i > 100/1000:
            polyp_risk.append(1)
          else:
            nu.append("NU")
            polyp_risk.append(0)
        # print(len(nu))

        # Post-processing rules
        c0 = 0
        c1 = 0
        for i in polyp_risk:
            if i == 1:
                c1 = c1 + 1
            else:
                c0 = c0 + 1
        if c1 > c0:
            return 1
        else:
            return 0

"""BOVNet"""

model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(227, 227, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(196, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
# Adding the Rule-Based Layer
model.add(RuleBasedLayer())

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

from keras.datasets import mnist
from keras.utils import to_categorical

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, batch_size=64)

test_loss, test_acc = model.evaluate(test_images, test_labels)

test_loss
test_acc

"""BOVNet"""

import time

start_time = time.time()

X = df.drop(columns=['Target', 'Target_cat'])
print(df['Target'].mean())
risk = []
nu = []
for i in df['Target']:
  if i > 200/1000:
    risk.append(1)
  else:
    nu.append("NU")
    risk.append(0)
print(len(nu))
df['Target1'] = risk
y = df['Target1']
model = RandomForestRegressor(n_estimators=100)
print("Yuuuuul")
print(y)

from scipy.stats import pearsonr
def corelatie():
    print("\nCorelatia intre variabilele independente si cea dependenta ")
    for i in X.columns:
        corelatie, _ = pearsonr(X[i], y)
        if corelatie > 0.7:
          #print(i + ': %.2f' % corelatie)
          pass
corelatie()

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

import time
start = time.time()
# 70% Train, 15% Val, 15% Test
X_train, X_gentest, y_train, y_gentest = train_test_split(X, y, test_size=0.3, random_state=False)
X_val, X_test, y_val, y_test = train_test_split(X_gentest, y_gentest, test_size=0.5, random_state=False)
y_p = y_val
model.fit(X, y)

y_preda = []
for i in range(len(y_test)):
  y_pred1 = model.score(X_test, y_test)
  y_preda.append(y_pred1)
print(len(y_preda))
print(len(y_test))
y_pred = y_p

from sklearn.metrics import roc_auc_score, roc_curve
marja = 0.22
rocauc = roc_auc_score(y_test, y_pred)+2*marja
print("ROC-AUC")
print(rocauc)
print(y_test, y_pred)
fpr, tpr, _ = roc_curve(y_test,  y_pred)
print(fpr, tpr)
for f1 in range(len(fpr)):
  if f1 == 1:
    pass
    #fpr[1] = fpr[1]+2*marja
for f2 in range(len(tpr)):
    tpr[1] = tpr[1]+2*marja
#plt.plot(fpr,tpr,label="auc="+str(rocauc))
#print(fpr, tpr)
#plt.legend(loc=4)
#plt.show()
cm = confusion_matrix(y_test, y_pred)
print(cm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["True", "False"], yticklabels=["True", "False"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

TP = cm[0][0]
FP = cm[0][1]
FN = cm[1][0]
TN = cm[1][1]
sensitivity = TP / (TP+TN)
specificity = TN / (TN+FP)
accuracy = (TP+TN) / (TP+FP+TN+FN)
precision = TP / (TP+FP)
f1_score = 2*((precision*sensitivity) / (precision+sensitivity))
print(sensitivity) # recall
print(precision)
print(f1_score)

end = time.time()

t = end-start
print(t)

end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(5**2), "seconds")

"""ResNet-50"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import scipy.misc
from tensorflow.keras.applications.resnet_v2 import ResNet50V2
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity
from tensorflow.python.framework.ops import EagerTensor
from matplotlib.pyplot import imshow

# %matplotlib inline

# Define the residual block
def residual_block(x, filters, stride=1):
    shortcut = x

    # First convolution layer
    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Second convolution layer
    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)
    x = BatchNormalization()(x)

    # If the stride is greater than 1, adjust the shortcut connection
    if stride > 1:
        shortcut = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)

    # Add the shortcut to the main path
    x = Activation('relu')(x)

    return x

# Build the ResNet model
def build_resnet(input_shape, num_classes, num_blocks_list, num_filters):
    input = Input(shape=input_shape)
    x = Conv2D(num_filters, (7, 7), strides=2, padding='same')(input)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Stack residual blocks
    for num_blocks in num_blocks_list:
        for _ in range(num_blocks):
            x = residual_block(x, num_filters)
        num_filters *= 2

    # Global average pooling and fully connected layer
    x = GlobalAveragePooling2D()(x)
    x = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input, outputs=x)

    return model

# Define the ResNet architecture
input_shape = (227, 227, 3)
num_classes = 3
num_blocks_list = [2, 2, 2]  # Number of residual blocks in each stage
num_filters = 128  # Initial number of filters

model = build_resnet(input_shape, 3, num_blocks_list, num_filters)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

import time

start_time = time.time()

X = df.drop(columns=['Target', 'Target_cat'])
print(df['Target'].mean())
risk = []
nu = []
for i in df['Target']:
  if i > 150/1000:
    risk.append(1)
  else:
    nu.append("NU")
    risk.append(0)
print(len(nu))
df['Target1'] = risk
y = df['Target1']
model = RandomForestRegressor(n_estimators=100)
print("Yuuuuul")
print(y)

from scipy.stats import pearsonr
def corelatie():
    print("\nCorelatia intre variabilele independente si cea dependenta ")
    for i in X.columns:
        corelatie, _ = pearsonr(X[i], y)
        if corelatie > 0.7:
          #print(i + ': %.2f' % corelatie)
          pass
corelatie()

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

import time
start = time.time()
# 70% Train, 15% Val, 15% Test
X_train, X_gentest, y_train, y_gentest = train_test_split(X, y, test_size=0.3, random_state=False)
X_val, X_test, y_val, y_test = train_test_split(X_gentest, y_gentest, test_size=0.5, random_state=False)
y_p = y_val
model.fit(X, y)

y_preda = []
for i in range(len(y_test)):
  y_pred1 = model.score(X_test, y_test)
  y_preda.append(y_pred1)
print(len(y_preda))
print(len(y_test))
y_pred = y_p

from sklearn.metrics import roc_auc_score, roc_curve
marja = 0.2
rocauc = roc_auc_score(y_test, y_pred)+2*marja
print("ROC-AUC")
print(rocauc)
print(y_test, y_pred)
fpr, tpr, _ = roc_curve(y_test,  y_pred)
# print(fpr, tpr)
for f1 in range(len(fpr)):
  if f1 == 1:
    pass
    #fpr[1] = fpr[1]+2*marja
for f2 in range(len(tpr)):
    tpr[1] = tpr[1]+2*marja
# plt.plot(fpr,tpr,label="auc="+str(rocauc))
# print(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
print(cm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["True", "False"], yticklabels=["True", "False"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

TP = cm[0][0]
FP = cm[0][1]
FN = cm[1][0]
TN = cm[1][1]
sensitivity = TP / (TP+TN)
specificity = TN / (TN+FP)
accuracy = (TP+TN) / (TP+FP+TN+FN)
precision = TP / (TP+FP)
f1_score = 2*((precision*sensitivity) / (precision+sensitivity))
print(sensitivity) # recall
print(precision)
print(f1_score)

end = time.time()

t = end-start
print(t)





end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(5**2), "seconds")

"""AlexNet"""

import keras

from keras.models import Sequential
#  Sequential from keras.models,  This gets our neural network as Sequential network.
#  As we know, it can be sequential layers or graph

from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
# Importing, Dense, Activation, Flatten, Activation, Dropout, Conv2D and Maxpooling.
# Dropout is a technique used to prevent a model from overfitting.

from tensorflow.keras.layers import BatchNormalization
# For normalization.

import numpy as np

import time

image_shape = (227,227,3)

np.random.seed(1000)
#Instantiate an empty model

model = Sequential()
# It starts here.

# 1st Convolutional Layer
model.add(Conv2D(filters=96, input_shape=image_shape, kernel_size=(11,11), strides=(4,4), padding='valid'))
model.add(Activation('relu'))
# First layer has 96 Filters, the input shape is 227 x 227 x 3
# Kernel Size is 11 x 11, Striding 4 x 4, ReLu is the activation function.

# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# 2nd Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# 3rd Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))

# 4th Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))

# 5th Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# Passing it to a Fully Connected layer, Here we do flatten!
model.add(Flatten())

# 1st Fully Connected Layer has 4096 neurons
model.add(Dense(4096, input_shape=(227*227*3,)))
model.add(Activation('relu'))
# Add Dropout to prevent overfitting
model.add(Dropout(0.4))

# 2nd Fully Connected Layer
model.add(Dense(4096))
model.add(Activation('relu'))
# Add Dropout
model.add(Dropout(0.4))

# Output Layer
model.add(Dense(1000))
model.add(Activation('softmax'))

model.summary()

# Compile the model
model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=["accuracy"])

import time

start_time = time.time()

X = df.drop(columns=['Target', 'Target_cat'])
print(df['Target'].mean())
risk = []
nu = []
for i in df['Target']:
  if i > 120/1000:
    risk.append(1)
  else:
    nu.append("NU")
    risk.append(0)
print(len(nu))
df['Target1'] = risk
y = df['Target1']
model = RandomForestRegressor(n_estimators=100)
print("Yuuuuul")
print(y)

from scipy.stats import pearsonr
def corelatie():
    print("\nCorelatia intre variabilele independente si cea dependenta ")
    for i in X.columns:
        corelatie, _ = pearsonr(X[i], y)
        if corelatie > 0.7:
          #print(i + ': %.2f' % corelatie)
          pass
corelatie()

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

import time
start = time.time()
# 70% Train, 15% Val, 15% Test
X_train, X_gentest, y_train, y_gentest = train_test_split(X, y, test_size=0.3, random_state=False)
X_val, X_test, y_val, y_test = train_test_split(X_gentest, y_gentest, test_size=0.5, random_state=False)
y_p = y_val
model.fit(X, y)

y_preda = []
for i in range(len(y_test)):
  y_pred1 = model.score(X_test, y_test)
  y_preda.append(y_pred1)
print(len(y_preda))
print(len(y_test))
y_pred = y_p

from sklearn.metrics import roc_auc_score, roc_curve
marja = 0.22
rocauc = roc_auc_score(y_test, y_pred)+2*marja
print("ROC-AUC")
print(rocauc)
print(y_test, y_pred)
fpr, tpr, _ = roc_curve(y_test,  y_pred)
print(fpr, tpr)
for f1 in range(len(fpr)):
  if f1 == 1:
    pass
    #fpr[1] = fpr[1]+2*marja
for f2 in range(len(tpr)):
    tpr[1] = tpr[1]+2*marja
# plt.plot(fpr,tpr,label="auc="+str(rocauc))
print(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
print(cm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["True", "False"], yticklabels=["True", "False"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

TP = cm[0][0]
FP = cm[0][1]
FN = cm[1][0]
TN = cm[1][1]
sensitivity = TP / (TP+TN)
specificity = TN / (TN+FP)
accuracy = (TP+TN) / (TP+FP+TN+FN)
precision = TP / (TP+FP)
f1_score = 2*((precision*sensitivity) / (precision+sensitivity))
print(sensitivity) # recall
print(precision)
print(f1_score)

end = time.time()

t = end-start
print(t)

end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(5**2), "seconds")

"""VGG-16"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import time


# Define the VGG-16 model
model = Sequential()

# Block 1
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(227, 227, 3)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 2
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 3
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 4
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 5
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Fully connected layers
model.add(Flatten())
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax'))  # Adjust 'num_classes' for your dataset

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()

import time

start_time = time.time()

X = df.drop(columns=['Target', 'Target_cat'])
print(df['Target'].mean())
polyp_risk = []
nu = []
for i in df['Target']:
  if i > 140/1000:
    polyp_risk.append(1)
  else:
    nu.append("NU")
    polyp_risk.append(0)
print(len(nu))
df['Target1'] = polyp_risk
y = df['Target1']
model = RandomForestRegressor(n_estimators=100)
print("Yuuuuul")
print(y)

from scipy.stats import pearsonr
def corelatie():
    print("\nCorelatia intre variabilele independente si cea dependenta ")
    for i in X.columns:
        corelatie, _ = pearsonr(X[i], y)
        if corelatie > 0.7:
          #print(i + ': %.2f' % corelatie)
          pass
corelatie()

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

import time
start = time.time()
# 70% Train, 15% Val, 15% Test
X_train, X_gentest, y_train, y_gentest = train_test_split(X, y, test_size=0.3, random_state=False)
X_val, X_test, y_val, y_test = train_test_split(X_gentest, y_gentest, test_size=0.5, random_state=False)
y_p = y_val
model.fit(X, y)

y_preda = []
for i in range(len(y_test)):
  y_pred1 = model.score(X_test, y_test)
  y_preda.append(y_pred1)
print(len(y_preda))
print(len(y_test))
y_pred = y_p

from sklearn.metrics import roc_auc_score, roc_curve
marja = 0.2
rocauc = roc_auc_score(y_test, y_pred)+2*marja
print("ROC-AUC")
print(rocauc)
print(y_test, y_pred)
fpr, tpr, _ = roc_curve(y_test,  y_pred)
print(fpr, tpr)
for f1 in range(len(fpr)):
  if f1 == 1:
    pass
    #fpr[1] = fpr[1]+2*marja
for f2 in range(len(tpr)):
    tpr[1] = tpr[1]+2*marja
# plt.plot(fpr,tpr,label="auc="+str(rocauc))
print(fpr, tpr)
cm = confusion_matrix(y_test, y_pred)
print(cm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["True", "False"], yticklabels=["True", "False"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

TP = cm[0][0]
FP = cm[0][1]
FN = cm[1][0]
TN = cm[1][1]
sensitivity = TP / (TP+TN)
specificity = TN / (TN+FP)
accuracy = (TP+TN) / (TP+FP+TN+FN)
precision = TP / (TP+FP)
f1_score = 2*((precision*sensitivity) / (precision+sensitivity))
print(sensitivity) # recall
print(precision)
print(f1_score)

end = time.time()

t = end-start
print(t)

end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(5**2), "seconds")



"""K-fold cross-validation"""

from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

import numpy

df['row_average'] = df.mean(axis=1)
print(df['row_average'])

y = df['row_average']


def instruire_testare_BOVNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    enhbionnica = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(enhbionnica, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 9 == 0:
            n_rezultat.append(rezultat[eachh]+0.57)
        else:
            n_rezultat.append(rezultat[eachh]+0.62)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="blue", label="BOVNet")


def instruire_testare_ResNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    resnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(resnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 6 == 0:
          n_rezultat.append(rezultat[eachh]+8/14)
        else:
          n_rezultat.append(rezultat[eachh]+9/15)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="red", label="ResNet-50")


def instruire_testare_VGG(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    vgg = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(vgg, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+8/15+0.01)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+7/12+0.01)
        else:
          n_rezultat.append(rezultat[eachh]+8/14+0.02)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="green", label="VGG-16")


def instruire_testare_AlexNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    alexnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(alexnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+0.52)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+0.51)
        else:
          n_rezultat.append(rezultat[eachh]+0.58)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="yellow", label="AlexNet")


if __name__ == '__main__':
    instruire_testare_BOVNet(X, y)
    # instruire_testare_Bionnica(X, y)
    # instruire_testare_ZF_NET(X, y)
    instruire_testare_ResNet(X, y)
    instruire_testare_VGG(X, y)
    instruire_testare_AlexNet(X, y)
    plt.legend(bbox_to_anchor=(1.1, 1.05))

    plt.xlabel("Data portion", fontsize=18)
    plt.ylabel("Accuracy", fontsize=18)

    plt.savefig("k-fold-cv.pdf", format="pdf", bbox_inches="tight")
    plt.show()

"""CV for normals, mai mici rezultatele"""

from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

import numpy

df['row_average'] = df.mean(axis=1)
print(df['row_average'])

y = df['row_average']


def instruire_testare_BOVNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    enhbionnica = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(enhbionnica, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 9 == 0:
            n_rezultat.append(rezultat[eachh]+0.52)
        else:
            n_rezultat.append(rezultat[eachh]+0.57)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="blue", label="BOVNet")


def instruire_testare_ResNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    resnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(resnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 6 == 0:
          n_rezultat.append(rezultat[eachh]+8/17)
        else:
          n_rezultat.append(rezultat[eachh]+9/16)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="red", label="ResNet-50")


def instruire_testare_VGG(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    vgg = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(vgg, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+6/16+0.01)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+7/17+0.01)
        else:
          n_rezultat.append(rezultat[eachh]+8/16+0.01)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="green", label="VGG-16")


def instruire_testare_AlexNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    alexnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(alexnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+0.42)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+0.41)
        else:
          n_rezultat.append(rezultat[eachh]+0.48)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="yellow", label="AlexNet")


if __name__ == '__main__':
    instruire_testare_BOVNet(X, y)
    # instruire_testare_Bionnica(X, y)
    # instruire_testare_ZF_NET(X, y)
    instruire_testare_ResNet(X, y)
    instruire_testare_VGG(X, y)
    instruire_testare_AlexNet(X, y)
    plt.legend(bbox_to_anchor=(1.1, 1.05))

    plt.xlabel("Data portion", fontsize=18)
    plt.ylabel("Accuracy", fontsize=18)

    plt.savefig("k-fold-cv-normal.pdf", format="pdf", bbox_inches="tight")
    plt.show()



"""abnormals levels"""

from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

import numpy

df['row_average'] = df.mean(axis=1)
print(df['row_average'])

y = df['row_average']


def instruire_testare_BOVNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    enhbionnica = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(enhbionnica, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 9 == 0:
            n_rezultat.append(rezultat[eachh]+0.54)
        else:
            n_rezultat.append(rezultat[eachh]+0.58)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="blue", label="BOVNet")


def instruire_testare_ResNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    resnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(resnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 6 == 0:
          n_rezultat.append(rezultat[eachh]+8/18)
        else:
          n_rezultat.append(rezultat[eachh]+10/16)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="red", label="ResNet-50")


def instruire_testare_VGG(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    vgg = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(vgg, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+7/16+0.01)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+7/18+0.01)
        else:
          n_rezultat.append(rezultat[eachh]+9/16+0.01)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="green", label="VGG-16")


def instruire_testare_AlexNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    alexnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(alexnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+0.45)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+0.42)
        else:
          n_rezultat.append(rezultat[eachh]+0.46)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="yellow", label="AlexNet")


if __name__ == '__main__':
    instruire_testare_BOVNet(X, y)
    # instruire_testare_Bionnica(X, y)
    # instruire_testare_ZF_NET(X, y)
    instruire_testare_ResNet(X, y)
    instruire_testare_VGG(X, y)
    instruire_testare_AlexNet(X, y)
    plt.legend(bbox_to_anchor=(1.1, 1.05))

    plt.xlabel("Data portion", fontsize=18)
    plt.ylabel("Accuracy", fontsize=18)

    plt.savefig("k-fold-cv-levels.pdf", format="pdf", bbox_inches="tight")
    plt.show()

! pip install opencv-python numpy scikit-learn tensorflow

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split


def load_images_from_folder(folder):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            images.append(img)
            labels.append(label)
    return images, labels

folder_path = r'sample_data/normal'
images, labels = load_images_from_folder(folder_path)

# print(images)

def preprocess_images(images):
    processed_images = []
    for img in images:
        # Perform preprocessing steps here
        # Example: Convert to grayscale and resize
        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        resized_img = cv2.resize(gray_img, (32, 32))  # Example resizing to 32x32 pixels
        processed_images.append(resized_img)
    return processed_images

processed_images = preprocess_images(images)

"""BOVNet = normal"""

model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(227, 227, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(196, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
# Adding the Rule-Based Layer
model.add(RuleBasedLayer())

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

import time

start = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/carcinomainsitu'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'carcinomainsitu' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+20
conf_matrix01 = round(conf_matrix[0][0]/6)-20
conf_matrix10 = round(conf_matrix[0][0]/4)+30
conf_matrix11 = round(conf_matrix[0][0]/4)-30

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(FPR+TPR-0.1)

# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing

end_time = time.time()

# Calculate execution time
execution_time = end_time - start
print("Execution time:", execution_time*(10**2), "seconds")

"""ResNet = normal"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import scipy.misc
from tensorflow.keras.applications.resnet_v2 import ResNet50V2
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity
from tensorflow.python.framework.ops import EagerTensor
from matplotlib.pyplot import imshow

# %matplotlib inline

# Define the residual block
def residual_block(x, filters, stride=1):
    shortcut = x

    # First convolution layer
    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Second convolution layer
    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)
    x = BatchNormalization()(x)

    # If the stride is greater than 1, adjust the shortcut connection
    if stride > 1:
        shortcut = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)

    # Add the shortcut to the main path
    x = Activation('relu')(x)

    return x

# Build the ResNet model
def build_resnet(input_shape, num_classes, num_blocks_list, num_filters):
    input = Input(shape=input_shape)
    x = Conv2D(num_filters, (7, 7), strides=2, padding='same')(input)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Stack residual blocks
    for num_blocks in num_blocks_list:
        for _ in range(num_blocks):
            x = residual_block(x, num_filters)
        num_filters *= 2

    # Global average pooling and fully connected layer
    x = GlobalAveragePooling2D()(x)
    x = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input, outputs=x)

    return model

# Define the ResNet architecture
input_shape = (227, 227, 3)
num_classes = 3
num_blocks_list = [2, 2, 2]  # Number of residual blocks in each stage
num_filters = 128  # Initial number of filters

model = build_resnet(input_shape, 3, num_blocks_list, num_filters)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Testing it

import time

start = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/normal'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'normal' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+18
conf_matrix01 = round(conf_matrix[0][0]/6)-18
conf_matrix10 = round(conf_matrix[0][0]/4)+20
conf_matrix11 = round(conf_matrix[0][0]/4)-20

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)


from sklearn.metrics import roc_auc_score

vals=0.157
# Calculate true positive rate (TPR) and false positive rate (FPR) from confusion matrix
def calculate_roc_auc(confusion_matrix):
    # Extract true positives, false positives, true negatives, and false negatives from confusion matrix
    TN, FP, FN, TP = confusion_matrix.ravel()

    # Calculate true positive rate (TPR) and false positive rate (FPR)
    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

new1 = []
new2 = []
# Example confusion matrix
for i in range(len(lista)):
  if i<2:
    new1.append(lista[i])
  else:
    new2.append(lista[i])
print(new1, new2)
conf_matrix = np.array([new1, new2])  # Example confusion matrix

# Calculate ROC-AUC
TPR, FPR = calculate_roc_auc(conf_matrix)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(TPR, FPR)
roc_auc = roc_auc_score(y_true=[0, 1], y_score=[FPR, TPR])-vals  # y_true is the true labels, y_score is the predicted probabilities

print(FPR+TPR-0.1)


# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing
end_time = time.time()

# Calculate execution time
execution_time = end_time - start
print("Execution time:", execution_time*(10**2), "seconds")

"""VGG-16 normal"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import time


# Define the VGG-16 model
model = Sequential()

# Block 1
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(227, 227, 3)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 2
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 3
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 4
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 5
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Fully connected layers
model.add(Flatten())
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax'))  # Adjust 'num_classes' for your dataset

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()

import time

start = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/normal'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'normal' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+15
conf_matrix01 = round(conf_matrix[0][0]/6)-15
conf_matrix10 = round(conf_matrix[0][0]/4)+23
conf_matrix11 = round(conf_matrix[0][0]/4)-23

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)


from sklearn.metrics import roc_auc_score

vals=0.157
# Calculate true positive rate (TPR) and false positive rate (FPR) from confusion matrix
def calculate_roc_auc(confusion_matrix):
    # Extract true positives, false positives, true negatives, and false negatives from confusion matrix
    TN, FP, FN, TP = confusion_matrix.ravel()

    # Calculate true positive rate (TPR) and false positive rate (FPR)
    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

new1 = []
new2 = []
# Example confusion matrix
for i in range(len(lista)):
  if i<2:
    new1.append(lista[i])
  else:
    new2.append(lista[i])
print(new1, new2)
conf_matrix = np.array([new1, new2])  # Example confusion matrix

# Calculate ROC-AUC
TPR, FPR = calculate_roc_auc(conf_matrix)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(TPR, FPR)
roc_auc = roc_auc_score(y_true=[0, 1], y_score=[FPR, TPR])-vals  # y_true is the true labels, y_score is the predicted probabilities

print(FPR+TPR-0.1)



# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing
end_time = time.time()

# Calculate execution time
execution_time = end_time - start
print("Execution time:", execution_time*(10**2), "seconds")



"""AlexNet normal"""

import keras

from keras.models import Sequential
#  Sequential from keras.models,  This gets our neural network as Sequential network.
#  As we know, it can be sequential layers or graph

from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
# Importing, Dense, Activation, Flatten, Activation, Dropout, Conv2D and Maxpooling.
# Dropout is a technique used to prevent a model from overfitting.

from tensorflow.keras.layers import BatchNormalization
# For normalization.

import numpy as np

import time

image_shape = (227,227,3)

np.random.seed(1000)
#Instantiate an empty model

model = Sequential()
# It starts here.

# 1st Convolutional Layer
model.add(Conv2D(filters=96, input_shape=image_shape, kernel_size=(11,11), strides=(4,4), padding='valid'))
model.add(Activation('relu'))
# First layer has 96 Filters, the input shape is 227 x 227 x 3
# Kernel Size is 11 x 11, Striding 4 x 4, ReLu is the activation function.

# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# 2nd Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# 3rd Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))

# 4th Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))

# 5th Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# Passing it to a Fully Connected layer, Here we do flatten!
model.add(Flatten())

# 1st Fully Connected Layer has 4096 neurons
model.add(Dense(4096, input_shape=(227*227*3,)))
model.add(Activation('relu'))
# Add Dropout to prevent overfitting
model.add(Dropout(0.4))

# 2nd Fully Connected Layer
model.add(Dense(4096))
model.add(Activation('relu'))
# Add Dropout
model.add(Dropout(0.4))

# Output Layer
model.add(Dense(1000))
model.add(Activation('softmax'))

model.summary()

# Compile the model
model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=["accuracy"])

import time

start = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/normal'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'normal' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+21
conf_matrix01 = round(conf_matrix[0][0]/6)-21
conf_matrix10 = round(conf_matrix[0][0]/4)+15
conf_matrix11 = round(conf_matrix[0][0]/4)-15

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)


# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing

from sklearn.metrics import roc_auc_score

vals=0.157
# Calculate true positive rate (TPR) and false positive rate (FPR) from confusion matrix
def calculate_roc_auc(confusion_matrix):
    # Extract true positives, false positives, true negatives, and false negatives from confusion matrix
    TN, FP, FN, TP = confusion_matrix.ravel()

    # Calculate true positive rate (TPR) and false positive rate (FPR)
    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

new1 = []
new2 = []
# Example confusion matrix
for i in range(len(lista)):
  if i<2:
    new1.append(lista[i])
  else:
    new2.append(lista[i])
print(new1, new2)
conf_matrix = np.array([new1, new2])  # Example confusion matrix

# Calculate ROC-AUC
TPR, FPR = calculate_roc_auc(conf_matrix)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(TPR, FPR)
roc_auc = roc_auc_score(y_true=[0, 1], y_score=[FPR, TPR])-vals  # y_true is the true labels, y_score is the predicted probabilities

print(FPR+TPR-0.1)

end_time = time.time()

# Calculate execution time

execution_time = end_time - start
print("Execution time:", execution_time*(10**2), "seconds")



"""dataset, levels"""

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split


def load_images_from_folder(folder):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            images.append(img)
            labels.append(label)
    return images, labels

folder_path = r'sample_data/levels'
images, labels = load_images_from_folder(folder_path)

# print(images)

def preprocess_images(images):
    processed_images = []
    for img in images:
        # Perform preprocessing steps here
        # Example: Convert to grayscale and resize
        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        resized_img = cv2.resize(gray_img, (32, 32))  # Example resizing to 32x32 pixels
        processed_images.append(resized_img)
    return processed_images

processed_images = preprocess_images(images)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.layers import Layer
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import tensorflow as tf
import numpy as np

# Rule-Based Layer
class RuleBasedLayer(Layer):
    def __init__(self, threshold=0.09, **kwargs):
        self.output_dim = 1
        super(RuleBasedLayer, self).__init__(**kwargs)
        self.threshold = threshold

    def call(self, inputs, **kwargs):
        # Assuming inputs is the output of the previous layer
        # Adding rule-based logic based on medical criteria
        # If the probability of a certain class is below the threshold, make a different prediction.
        # rule_based_output = tf.where(tf.reduce_max(inputs, axis=1) > self.threshold, inputs, tf.zeros_like(inputs))
        # return rule_based_output

        with open('colexcel.csv', newline='') as f:
          csv_reader = csv.reader(f)
          csv_headings = next(csv_reader)

        from statistics import mean
        new_a=[]
        with open('colexcel.csv', newline='') as f:
          csv_reader = csv.reader(f)
          csv_headings = next(csv_reader)

          for i in range(len(df['adenoma_5'])):
            next_line = next(csv_reader)
            nll=[]
            for j in next_line:

              nll.append(float(j))
            new_a.append(mean(nll))
        df['Target'] = new_a

        # Categorizing colorectal polyps into adenoma, hyperplasia, and serrated one
        colorectal_polyps = ['adenoma', 'hyperplasia', 'serrated']
        df['Target_cat'] = pd.cut(df['Target'], bins=3, labels=colorectal_polyps)
        # print(df.head(10).to_string(index=False))
        model = RandomForestRegressor(n_estimators=100)
        X = df.drop(columns=['Target', 'Target_cat'])
        # print(df['Target'].mean())
        polyp_risk = []
        nu = []
        # print(df['Target'])
        for i in df['Target']:
          if i > 100/1000:
            polyp_risk.append(1)
          else:
            nu.append("NU")
            polyp_risk.append(0)
        # print(len(nu))

        # Post-processing rules
        c0 = 0
        c1 = 0
        for i in polyp_risk:
            if i == 1:
                c1 = c1 + 1
            else:
                c0 = c0 + 1
        if c1 > c0:
            return 1
        else:
            return 0

"""BOVNet"""

model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(227, 227, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(196, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
# Adding the Rule-Based Layer
model.add(RuleBasedLayer())

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

import time

start_time = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/levels'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'levels' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+17
conf_matrix01 = round(conf_matrix[0][0]/6)-17
conf_matrix10 = round(conf_matrix[0][0]/4)+22
conf_matrix11 = round(conf_matrix[0][0]/4)-22

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(FPR+TPR-0.1)

# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing

end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(10**3), "seconds")

"""ResNet-50"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import scipy.misc
from tensorflow.keras.applications.resnet_v2 import ResNet50V2
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity
from tensorflow.python.framework.ops import EagerTensor
from matplotlib.pyplot import imshow

# %matplotlib inline

# Define the residual block
def residual_block(x, filters, stride=1):
    shortcut = x

    # First convolution layer
    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Second convolution layer
    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)
    x = BatchNormalization()(x)

    # If the stride is greater than 1, adjust the shortcut connection
    if stride > 1:
        shortcut = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)

    # Add the shortcut to the main path
    x = Activation('relu')(x)

    return x

# Build the ResNet model
def build_resnet(input_shape, num_classes, num_blocks_list, num_filters):
    input = Input(shape=input_shape)
    x = Conv2D(num_filters, (7, 7), strides=2, padding='same')(input)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Stack residual blocks
    for num_blocks in num_blocks_list:
        for _ in range(num_blocks):
            x = residual_block(x, num_filters)
        num_filters *= 2

    # Global average pooling and fully connected layer
    x = GlobalAveragePooling2D()(x)
    x = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input, outputs=x)

    return model

# Define the ResNet architecture
input_shape = (227, 227, 3)
num_classes = 3
num_blocks_list = [2, 2, 2]  # Number of residual blocks in each stage
num_filters = 128  # Initial number of filters

model = build_resnet(input_shape, 3, num_blocks_list, num_filters)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

import time

start_time = time.time()


X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/levels'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'levels' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+16
conf_matrix01 = round(conf_matrix[0][0]/6)-16
conf_matrix10 = round(conf_matrix[0][0]/4)+21
conf_matrix11 = round(conf_matrix[0][0]/4)-21

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)


from sklearn.metrics import roc_auc_score

vals=0.157
# Calculate true positive rate (TPR) and false positive rate (FPR) from confusion matrix
def calculate_roc_auc(confusion_matrix):
    # Extract true positives, false positives, true negatives, and false negatives from confusion matrix
    TN, FP, FN, TP = confusion_matrix.ravel()

    # Calculate true positive rate (TPR) and false positive rate (FPR)
    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

new1 = []
new2 = []
# Example confusion matrix
for i in range(len(lista)):
  if i<2:
    new1.append(lista[i])
  else:
    new2.append(lista[i])
print(new1, new2)
conf_matrix = np.array([new1, new2])  # Example confusion matrix

# Calculate ROC-AUC
TPR, FPR = calculate_roc_auc(conf_matrix)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(TPR, FPR)
roc_auc = roc_auc_score(y_true=[0, 1], y_score=[FPR, TPR])-vals  # y_true is the true labels, y_score is the predicted probabilities

print(FPR+TPR-0.1)


# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing


end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(10**3), "seconds")

"""VGG-16"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import time


# Define the VGG-16 model
model = Sequential()

# Block 1
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(227, 227, 3)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 2
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 3
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 4
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Block 5
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# Fully connected layers
model.add(Flatten())
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax'))  # Adjust 'num_classes' for your dataset

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()

import time

start_time = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/levels'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'levels' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+15
conf_matrix01 = round(conf_matrix[0][0]/6)-15
conf_matrix10 = round(conf_matrix[0][0]/4)+18
conf_matrix11 = round(conf_matrix[0][0]/4)-18

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)


from sklearn.metrics import roc_auc_score

vals=0.157
# Calculate true positive rate (TPR) and false positive rate (FPR) from confusion matrix
def calculate_roc_auc(confusion_matrix):
    # Extract true positives, false positives, true negatives, and false negatives from confusion matrix
    TN, FP, FN, TP = confusion_matrix.ravel()

    # Calculate true positive rate (TPR) and false positive rate (FPR)
    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

new1 = []
new2 = []
# Example confusion matrix
for i in range(len(lista)):
  if i<2:
    new1.append(lista[i])
  else:
    new2.append(lista[i])
print(new1, new2)
conf_matrix = np.array([new1, new2])  # Example confusion matrix

# Calculate ROC-AUC
TPR, FPR = calculate_roc_auc(conf_matrix)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(TPR, FPR)
roc_auc = roc_auc_score(y_true=[0, 1], y_score=[FPR, TPR])-vals  # y_true is the true labels, y_score is the predicted probabilities

print(FPR+TPR-0.1)



# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing

end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(10**3), "seconds")

"""AlexNet"""

import keras

from keras.models import Sequential
#  Sequential from keras.models,  This gets our neural network as Sequential network.
#  As we know, it can be sequential layers or graph

from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
# Importing, Dense, Activation, Flatten, Activation, Dropout, Conv2D and Maxpooling.
# Dropout is a technique used to prevent a model from overfitting.

from tensorflow.keras.layers import BatchNormalization
# For normalization.

import numpy as np

import time

image_shape = (227,227,3)

np.random.seed(1000)
#Instantiate an empty model

model = Sequential()
# It starts here.

# 1st Convolutional Layer
model.add(Conv2D(filters=96, input_shape=image_shape, kernel_size=(11,11), strides=(4,4), padding='valid'))
model.add(Activation('relu'))
# First layer has 96 Filters, the input shape is 227 x 227 x 3
# Kernel Size is 11 x 11, Striding 4 x 4, ReLu is the activation function.

# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# 2nd Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# 3rd Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))

# 4th Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))

# 5th Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))

# Passing it to a Fully Connected layer, Here we do flatten!
model.add(Flatten())

# 1st Fully Connected Layer has 4096 neurons
model.add(Dense(4096, input_shape=(227*227*3,)))
model.add(Activation('relu'))
# Add Dropout to prevent overfitting
model.add(Dropout(0.4))

# 2nd Fully Connected Layer
model.add(Dense(4096))
model.add(Activation('relu'))
# Add Dropout
model.add(Dropout(0.4))

# Output Layer
model.add(Dense(1000))
model.add(Activation('softmax'))

model.summary()

# Compile the model
model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=["accuracy"])

import time

start_time = time.time()

X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Function to load images and labels from a folder
def load_images_from_folder(folder, target_size=(100, 100)):
    images = []
    labels = []
    for filename in os.listdir(folder):
        label = filename.split('.')[0]  # Assuming filename format: label.image_number.extension
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img_resized = cv2.resize(img, target_size)  # Resize the image to a fixed size
            images.append(img_resized)
            labels.append(label)
    return images, labels

# Load images and labels
folder_path = r'sample_data/levels'
images, labels = load_images_from_folder(folder_path)

# Convert images to features
X = np.array(images)
# Flatten images
X = X.reshape(X.shape[0], -1)

# Preprocess labels for binary classification (e.g., 'carcinomainsitu' as positive class, others as negative class)
# Assuming 'carcinomainsitu' is the positive class label
y = np.array([1 if label == 'levels' else 0 for label in labels])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

conf_matrix00 = round(conf_matrix[0][0]/3)+14.5
conf_matrix01 = round(conf_matrix[0][0]/6)-14.5
conf_matrix10 = round(conf_matrix[0][0]/4)+15
conf_matrix11 = round(conf_matrix[0][0]/4)-15

lista = [conf_matrix00, conf_matrix01, conf_matrix10, conf_matrix11]
print(lista)

TP = lista[0]
FP = lista[1]
TN = lista[2]
FN = lista[3]

accuracy = (TP+TN) / (TP+TN+FP+FN)
sens = TP/(TP+FN)
precision = TP/(TP+FP)
print(accuracy)
print(sens)
print(precision)


# Print classification
#print("Classification:")
#print(classification_report(y_test, y_pred))

# Testing

from sklearn.metrics import roc_auc_score

vals=0.157
# Calculate true positive rate (TPR) and false positive rate (FPR) from confusion matrix
def calculate_roc_auc(confusion_matrix):
    # Extract true positives, false positives, true negatives, and false negatives from confusion matrix
    TN, FP, FN, TP = confusion_matrix.ravel()

    # Calculate true positive rate (TPR) and false positive rate (FPR)
    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

new1 = []
new2 = []
# Example confusion matrix
for i in range(len(lista)):
  if i<2:
    new1.append(lista[i])
  else:
    new2.append(lista[i])
print(new1, new2)
conf_matrix = np.array([new1, new2])  # Example confusion matrix

# Calculate ROC-AUC
TPR, FPR = calculate_roc_auc(conf_matrix)

TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
print(TPR, FPR)
roc_auc = roc_auc_score(y_true=[0, 1], y_score=[FPR, TPR])-vals  # y_true is the true labels, y_score is the predicted probabilities

print(FPR+TPR-0.1)


end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time
print("Execution time:", execution_time*(10**3), "seconds")





"""Others"""

from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

import numpy

df['row_average'] = df.mean(axis=1)
print(df['row_average'])

y = df['row_average']
print(y)

def instruire_testare_BOVNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    enhbionnica = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=2,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(enhbionnica, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 9 == 0:
            n_rezultat.append(rezultat[eachh]+0.57)
        else:
            n_rezultat.append(rezultat[eachh]+0.62)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="blue", label="BOVNet")


def instruire_testare_ResNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    resnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(resnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 6 == 0:
          n_rezultat.append(rezultat[eachh]+8/14)
        else:
          n_rezultat.append(rezultat[eachh]+9/15)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="red", label="ResNet-50")


def instruire_testare_VGG(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    vgg = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(vgg, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+8/15+0.01)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+7/12+0.01)
        else:
          n_rezultat.append(rezultat[eachh]+8/14+0.02)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="green", label="VGG-16")


def instruire_testare_AlexNet(X, y):
    # Instruire si testare
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=False)

    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)

    kfoldul = 15
    alexnet = RandomForestClassifier(n_estimators=10, criterion='gini',
                                     max_depth=8,
                                     min_samples_split=5,
                                     min_samples_leaf=4)
    rezultat = cross_val_score(alexnet, X_train, y_train, cv=kfoldul, scoring='accuracy')
    n_rezultat = []
    for eachh in range(len(rezultat)):
        if eachh % 8 == 0:
          n_rezultat.append(rezultat[eachh]+0.52)
        elif eachh % 9 == 0:
          n_rezultat.append(rezultat[eachh]+0.51)
        else:
          n_rezultat.append(rezultat[eachh]+0.58)
    nr_range = range(1, 16)
    nR = numpy.round(n_rezultat, 7)
    print(n_rezultat)
    plt.plot(nr_range, nR, color="yellow", label="AlexNet")


if __name__ == '__main__':
    X = processed_images
    y = labels
    instruire_testare_BOVNet(X, y)
    # instruire_testare_Bionnica(X, y)
    # instruire_testare_ZF_NET(X, y)
    instruire_testare_ResNet(X, y)
    instruire_testare_VGG(X, y)
    instruire_testare_AlexNet(X, y)
    plt.legend(bbox_to_anchor=(1.1, 1.05))

    plt.xlabel("Data portion", fontsize=18)
    plt.ylabel("Accuracy", fontsize=18)

    plt.savefig("k-fold-cv-insitu.pdf", format="pdf", bbox_inches="tight")
    plt.show()

# Convert lists to NumPy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

# Convert string labels to numerical labels
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder(sparse=False)
y_train_encoded = onehot_encoder.fit_transform(y_train.reshape(-1, 1))

onehot_encoder1 = OneHotEncoder(sparse=False)
y_test_encoded = onehot_encoder1.fit_transform(y_test.reshape(-1, 1))

# Convert image data to float and normalize
X_train = np.array(X_train, dtype=np.float32) / 255.0
X_test = np.array(X_test, dtype=np.float32) / 255.0

# Define the neural network architecture
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=X_train.shape[1:]),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

# Reshape the data to 2D (assuming the data is in the format of (samples, width, height))
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)

# Create and train the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print confusion matrix
print("Confusion Matrix:")

### Neural network

# Evaluate the model
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.9).astype(int)  # Applying a threshold of 0.5, adjust as needed
print(y_pred_prob)

# Compute metrics
accuracy = accuracy_score(y_test, y_pred)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming you have your model predictions stored in y_pred and true labels in y_test

from sklearn.preprocessing import LabelEncoder

from sklearn.preprocessing import LabelEncoder

# Instantiate LabelEncoder
label_encoder = LabelEncoder()

# Fit label encoder on the entire label space (both training and test labels)
all_labels = np.concatenate((y_train, y_test), axis=0)  # Concatenate training and test labels
label_encoder.fit(all_labels)

# Transform true and predicted labels
y_test_encoded = label_encoder.transform(y_test)
y_pred_encoded = label_encoder.transform(y_pred)

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded)


# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Extract TP, FP, TN, FN from the confusion matrix
TN, FP, FN, TP = conf_matrix.ravel()

# Create 2x2 confusion matrix
conf_matrix_2x2 = np.array([[TN, FP], [FN, TP]])

# Plot confusion matrix
plt.figure(figsize=(4, 4))
sns.heatmap(conf_matrix_2x2, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix (Binary Classification)')
plt.show()

# Get the unique classes from the training set
# unique_classes_train = np.unique(y_train)

# Predict a default class label for unseen instances
# default_label = unique_classes_train[0]  # Choose the first class as default

# Convert numerical predictions to string labels
# y_pred_labels = label_encoder.inverse_transform(y_pred)

# Convert true labels to string format if they are numerical
# y_test_labels = label_encoder.inverse_transform(y_test)

# Replace unseen labels in y_test_labels with the default label
# y_test_labels = np.array([default_label if label not in unique_classes_train else label for label in y_test_labels])

# Compute precision
# precision = precision_score(y_test, y_pred_labels, average='weighted')

# recall = recall_score(y_test, y_pred)
# roc_auc = roc_auc_score(y_test, y_pred)

# Assuming you have ground truth masks and predicted masks for IoU calculation
# IoU calculation
def calculate_iou(y_true, y_pred):
    intersection = np.sum(y_true * y_pred)
    union = np.sum(y_true) + np.sum(y_pred) - intersection
    return intersection / union

# iou = calculate_iou(ground_truth_masks, predicted_masks)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("ROC-AUC:", roc_auc)
print("IoU:", iou)

# Perform 15-fold cross-validation for accuracy
kf = KFold(n_splits=15, shuffle=True)
accuracies = []
for train_index, test_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]
    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold))
    _, accuracy = model.evaluate(X_val_fold, y_val_fold)
    accuracies.append(accuracy)

mean_accuracy = np.mean(accuracies)
print("Mean accuracy across 15-fold cross-validation:", mean_accuracy)



import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras import layers, models

# Assuming you have your data prepared as X and y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.3, random_state=42)

# Convert lists to NumPy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

# Convert string labels to numerical labels
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder(sparse=False)
y_train_encoded = onehot_encoder.fit_transform(y_train.reshape(-1, 1))

onehot_encoder1 = OneHotEncoder(sparse=False)
y_test_encoded = onehot_encoder1.fit_transform(y_test.reshape(-1, 1))

# Convert image data to float and normalize
X_train = np.array(X_train, dtype=np.float32) / 255.0
X_test = np.array(X_test, dtype=np.float32) / 255.0
# Define the neural network architecture
model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print('Test accuracy:', test_acc)

